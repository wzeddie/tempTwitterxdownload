#!/usr/bin/env python3
"""
SEOç€é™†é¡µæ•°æ®åˆ†æä»ªè¡¨æ¿
ç”¨äºç”ŸæˆSEOå…³é”®æŒ‡æ ‡çš„å¯è§†åŒ–æŠ¥å‘Š
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
from datetime import datetime, timedelta
import json
import requests
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class SEODataAnalyzer:
    """SEOæ•°æ®åˆ†æå™¨"""
    
    def __init__(self, data_source: str = 'sample'):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        Args:
            data_source: æ•°æ®æºç±»å‹ ('google_analytics', 'search_console', 'sample')
        """
        self.data_source = data_source
        self.landing_pages = [
            'twitter-video-downloader',
            'twitter-gif-download', 
            'x-video-download',
            'mobile-video-download'
        ]
        
        # ç›®æ ‡å…³é”®è¯
        self.target_keywords = {
            'twitter video downloader': 'twitter-video-downloader',
            'twitter gif download': 'twitter-gif-download',
            'x video download': 'x-video-download',
            'mobile twitter download': 'mobile-video-download'
        }
        
        self.colors = {
            'twitter-video-downloader': '#1DA1F2',
            'twitter-gif-download': '#8B5CF6', 
            'x-video-download': '#000000',
            'mobile-video-download': '#10B981'
        }

    def generate_sample_data(self) -> Dict:
        """ç”Ÿæˆç¤ºä¾‹æ•°æ®ç”¨äºæ¼”ç¤º"""
        
        # ç”Ÿæˆ30å¤©çš„æ—¥æœŸèŒƒå›´
        date_range = pd.date_range(
            start=datetime.now() - timedelta(days=30),
            end=datetime.now(),
            freq='D'
        )
        
        data = {
            'traffic': {},
            'rankings': {},
            'conversions': {},
            'performance': {}
        }
        
        # ç”Ÿæˆæµé‡æ•°æ®
        for page in self.landing_pages:
            base_traffic = np.random.randint(100, 500)
            trend = np.linspace(1, 1.5, len(date_range))  # ä¸Šå‡è¶‹åŠ¿
            noise = np.random.normal(0, 0.1, len(date_range))
            
            traffic = base_traffic * trend * (1 + noise)
            traffic = np.maximum(traffic, 0).astype(int)
            
            data['traffic'][page] = {
                'dates': date_range.strftime('%Y-%m-%d').tolist(),
                'organic_traffic': traffic.tolist(),
                'bounce_rate': np.random.uniform(0.4, 0.7, len(date_range)).tolist(),
                'session_duration': np.random.uniform(120, 300, len(date_range)).tolist()
            }
        
        # ç”Ÿæˆæ’åæ•°æ®
        for keyword, page in self.target_keywords.items():
            initial_rank = np.random.randint(30, 50)
            trend = np.linspace(0, -15, len(date_range))  # æ’åä¸Šå‡ï¼ˆæ•°å­—ä¸‹é™ï¼‰
            noise = np.random.normal(0, 2, len(date_range))
            
            rankings = initial_rank + trend + noise
            rankings = np.maximum(rankings, 1).astype(int)
            
            data['rankings'][keyword] = {
                'dates': date_range.strftime('%Y-%m-%d').tolist(),
                'rankings': rankings.tolist(),
                'search_volume': np.random.randint(8000, 50000),
                'competition': np.random.choice(['Low', 'Medium', 'High'])
            }
        
        # ç”Ÿæˆè½¬åŒ–æ•°æ®
        for page in self.landing_pages:
            traffic_data = data['traffic'][page]['organic_traffic']
            conversion_rate = np.random.uniform(0.02, 0.05, len(date_range))
            conversions = (np.array(traffic_data) * conversion_rate).astype(int)
            
            data['conversions'][page] = {
                'dates': date_range.strftime('%Y-%m-%d').tolist(),
                'conversions': conversions.tolist(),
                'conversion_rate': (conversion_rate * 100).tolist()
            }
        
        # ç”Ÿæˆæ€§èƒ½æ•°æ®
        for page in self.landing_pages:
            data['performance'][page] = {
                'lcp': np.random.uniform(1.5, 3.0),
                'fid': np.random.uniform(50, 150), 
                'cls': np.random.uniform(0.05, 0.15),
                'seo_score': np.random.randint(75, 95)
            }
        
        return data

    def create_traffic_dashboard(self, data: Dict) -> go.Figure:
        """åˆ›å»ºæµé‡åˆ†æä»ªè¡¨æ¿"""
        
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=(
                'æœ‰æœºæµé‡è¶‹åŠ¿', 'è·³å‡ºç‡å¯¹æ¯”',
                'ä¼šè¯æ—¶é•¿åˆ†å¸ƒ', 'é¡µé¢æµé‡å æ¯”'
            ),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"type": "pie"}]]
        )
        
        # 1. æœ‰æœºæµé‡è¶‹åŠ¿
        for page in self.landing_pages:
            page_data = data['traffic'][page]
            fig.add_trace(
                go.Scatter(
                    x=page_data['dates'],
                    y=page_data['organic_traffic'],
                    mode='lines+markers',
                    name=page.replace('-', ' ').title(),
                    line=dict(color=self.colors[page], width=2)
                ),
                row=1, col=1
            )
        
        # 2. è·³å‡ºç‡å¯¹æ¯”
        pages = []
        bounce_rates = []
        for page in self.landing_pages:
            pages.append(page.replace('-', ' ').title())
            bounce_rates.append(np.mean(data['traffic'][page]['bounce_rate']) * 100)
        
        fig.add_trace(
            go.Bar(
                x=pages,
                y=bounce_rates,
                name='å¹³å‡è·³å‡ºç‡',
                marker_color=[self.colors[page] for page in self.landing_pages]
            ),
            row=1, col=2
        )
        
        # 3. ä¼šè¯æ—¶é•¿åˆ†å¸ƒ
        all_durations = []
        page_labels = []
        for page in self.landing_pages:
            durations = data['traffic'][page]['session_duration']
            all_durations.extend(durations)
            page_labels.extend([page.replace('-', ' ').title()] * len(durations))
        
        fig.add_trace(
            go.Box(
                y=all_durations,
                x=page_labels,
                name='ä¼šè¯æ—¶é•¿',
                marker_color='lightblue'
            ),
            row=2, col=1
        )
        
        # 4. é¡µé¢æµé‡å æ¯”
        total_traffic = []
        labels = []
        for page in self.landing_pages:
            total = sum(data['traffic'][page]['organic_traffic'])
            total_traffic.append(total)
            labels.append(page.replace('-', ' ').title())
        
        fig.add_trace(
            go.Pie(
                labels=labels,
                values=total_traffic,
                name="æµé‡å æ¯”",
                marker_colors=[self.colors[page] for page in self.landing_pages]
            ),
            row=2, col=2
        )
        
        fig.update_layout(
            title_text="SEOç€é™†é¡µæµé‡åˆ†æä»ªè¡¨æ¿",
            height=800,
            showlegend=True
        )
        
        return fig

    def generate_weekly_report(self, data: Dict) -> str:
        """ç”Ÿæˆæ¯å‘¨æ•°æ®æŠ¥å‘Š"""
        
        report_date = datetime.now().strftime('%Y-%m-%d')
        
        # è®¡ç®—å…³é”®æŒ‡æ ‡
        total_traffic = sum([sum(data['traffic'][page]['organic_traffic']) for page in self.landing_pages])
        avg_conversion_rate = np.mean([np.mean(data['conversions'][page]['conversion_rate']) for page in self.landing_pages])
        avg_bounce_rate = np.mean([np.mean(data['traffic'][page]['bounce_rate']) for page in self.landing_pages])
        
        # æ’åå˜åŒ–åˆ†æ
        ranking_changes = {}
        for keyword in self.target_keywords.keys():
            rankings = data['rankings'][keyword]['rankings']
            change = rankings[0] - rankings[-1]  # æ­£å€¼è¡¨ç¤ºæ’åä¸Šå‡
            ranking_changes[keyword] = change
        
        report = f"""
# SEOç€é™†é¡µå‘¨æŠ¥ - {report_date}

## ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡æ¦‚è§ˆ

### æµé‡æ•°æ®
- **æ€»æœ‰æœºè®¿é—®é‡**: {total_traffic:,} æ¬¡
- **å¹³å‡è½¬åŒ–ç‡**: {avg_conversion_rate:.2f}%
- **å¹³å‡è·³å‡ºç‡**: {avg_bounce_rate:.1%}

### ğŸ” å…³é”®è¯æ’åå˜åŒ–
"""
        
        for keyword, change in ranking_changes.items():
            direction = "â†—ï¸" if change > 0 else "â†˜ï¸" if change < 0 else "â¡ï¸"
            report += f"- **{keyword}**: {direction} {abs(change)} ä½\n"
        
        report += f"""

### ğŸ¯ é¡µé¢è¡¨ç°åˆ†æ
"""
        
        for page in self.landing_pages:
            page_traffic = sum(data['traffic'][page]['organic_traffic'])
            page_conversion = np.mean(data['conversions'][page]['conversion_rate'])
            page_title = page.replace('-', ' ').title()
            
            report += f"""
#### {page_title}
- è®¿é—®é‡: {page_traffic:,} æ¬¡
- è½¬åŒ–ç‡: {page_conversion:.2f}%
- æ€§èƒ½å¾—åˆ†: {data['performance'][page]['seo_score']}/100
"""
        
        report += f"""

### ğŸ“ˆ è¶‹åŠ¿åˆ†æ
- æ•´ä½“æµé‡å‘ˆä¸Šå‡è¶‹åŠ¿
- å…³é”®è¯æ’åæŒç»­æ”¹å–„
- ç”¨æˆ·å‚ä¸åº¦ç¨³æ­¥æå‡

### ğŸ¯ ä¸‹å‘¨ä¼˜åŒ–é‡ç‚¹
1. ç»§ç»­ä¼˜åŒ–é¡µé¢åŠ è½½é€Ÿåº¦
2. å¢å¼ºç§»åŠ¨ç«¯ç”¨æˆ·ä½“éªŒ
3. æ‰©å±•é•¿å°¾å…³é”®è¯è¦†ç›–
4. æå‡å†…å®¹è´¨é‡å’Œç›¸å…³æ€§

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return report

    def export_dashboard(self, output_dir: str = 'reports'):
        """å¯¼å‡ºå®Œæ•´çš„æ•°æ®åˆ†æä»ªè¡¨æ¿"""
        
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ•°æ®
        data = self.generate_sample_data()
        
        # åˆ›å»ºæµé‡å›¾è¡¨
        traffic_fig = self.create_traffic_dashboard(data)
        
        # ä¿å­˜HTMLæ–‡ä»¶
        traffic_fig.write_html(f"{output_dir}/traffic_dashboard.html")
        
        # ç”Ÿæˆæ–‡å­—æŠ¥å‘Š
        weekly_report = self.generate_weekly_report(data)
        with open(f"{output_dir}/weekly_report.md", 'w', encoding='utf-8') as f:
            f.write(weekly_report)
        
        # ä¿å­˜åŸå§‹æ•°æ®
        with open(f"{output_dir}/raw_data.json", 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²å¯¼å‡ºåˆ° {output_dir}/ ç›®å½•")
        print(f"ğŸ“Š åŒ…å«æ–‡ä»¶:")
        print(f"   - traffic_dashboard.html (æµé‡åˆ†æ)")
        print(f"   - weekly_report.md (å‘¨æŠ¥)")
        print(f"   - raw_data.json (åŸå§‹æ•°æ®)")

def main():
    """ä¸»å‡½æ•°"""
    analyzer = SEODataAnalyzer()
    
    print("ğŸš€ SEOæ•°æ®åˆ†æä»ªè¡¨æ¿å¯åŠ¨ä¸­...")
    print("ğŸ“ˆ æ­£åœ¨ç”Ÿæˆç¤ºä¾‹æ•°æ®å’Œå›¾è¡¨...")
    
    # å¯¼å‡ºå®Œæ•´æŠ¥å‘Š
    analyzer.export_dashboard()
    
    print("\nğŸ¯ ä½¿ç”¨è¯´æ˜:")
    print("1. æ‰“å¼€ç”Ÿæˆçš„HTMLæ–‡ä»¶æŸ¥çœ‹äº¤äº’å¼å›¾è¡¨")
    print("2. æŸ¥çœ‹weekly_report.mdäº†è§£å…³é”®æŒ‡æ ‡")
    print("3. ä½¿ç”¨raw_data.jsonè¿›è¡Œè¿›ä¸€æ­¥åˆ†æ")
    print("\nğŸ“ å¯ä»¥æ ¹æ®å®é™…éœ€æ±‚ä¿®æ”¹æ•°æ®æºå’ŒæŒ‡æ ‡")

if __name__ == "__main__":
    main() 